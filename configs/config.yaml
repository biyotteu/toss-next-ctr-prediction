# =============================================
# Experiment / Paths
# =============================================
exp_name: QIN_V9ISH_BASELINE
seed: 42
output_dir: ./outputs
artifacts_dir: ./artifacts

paths:
  train_parquet: ./data/train.parquet
  test_parquet: ./data/test.parquet
  sample_submission: ./data/sample_submission.csv
# =============================================
# Data & Features
# =============================================
label_col: clicked
id_col: ID
seq_col: seq # comma‑separated ints as string
# If l_feat_14 is missing or sparse, fallback will be inventory_id
target_feature: l_feat_14
seq_max_len: 256
seq_vocab_size: 400000 # hashing buckets for seq tokens
# Drop constants / bad cols (I/O optimization)
force_drop_cols: [l_feat_20, l_feat_23]
# Numeric/Category handling
numeric_fillna: 0.0
category_hash_buckets: 100000 # hashing trick buckets for generic categorical columns

# =============================================
# Split (leakage‑free options)
# =============================================
split:
  method: stratified # one of: stratified, group_user, group_session, time
  group_key: user_id # used when method=group_user
  session_key: session_id # used when method=group_session
  time_col: event_time # pandas‑parsable datetime col when method=time
  time_val_ratio: 0.1 # last X% as validation if method=time

# (kept for backward compatibility)
val_ratio: 0.1
stratify_label: true
# =============================================
# Imbalance & Sampling
# =============================================
# Option A: pos_weight for BCEWithLogitsLoss (no downsampling)
use_neg_downsampling: false
neg_downsample_ratio: 0.2 # keep 20% of negatives within each batch (~1:10 pos:neg if base pos≈1.9%)
pos_weight: null # if null, auto‑compute by global class ratio
min_pos_per_batch: 16

# Hard Negative Mining (OHEM‑style)
hnm:
  enable: false
  top_neg_frac: 0.2 # keep hardest 20% negatives (by per‑example loss) in the batch
  min_neg: 256 # at least this many negatives kept if available

# =============================================
# DataLoader
# =============================================
persistent_workers: false
prefetch_factor: 2
num_workers: 16
pin_memory: true

# Dataset cache acceleration
dataset_cache: true               # 캐싱 켜기
dataset_cache_progress: true      # tqdm 진행 표시
dataset_cache_workers: 16         # 병렬 워커 수 (CPU 코어에 맞춰 조정)
dataset_cache_chunk_rows: 400000  # 시퀀스 처리 청크 크기
dataset_cache_backend: "ram"    # "memmap" | "ram"
dataset_cache_dir: ./artifacts/cache_seq  # 캐시 저장 경로(영구 재사용)
# 범주형 해시 방식 선택
#  - "fnv"     : 기존과 동일 매핑(체크포인트 호환), 병렬 파이썬(FNV) → 느리지만 동일
#  - "pandas"  : 매우 빠름(벡터화), 다만 해시 매핑 달라짐(재학습 필요)
dataset_cache_hash: "fnv"
# =============================================
# Optimization
# =============================================
batch_size: 8192    #4096 8192
epochs: 100
lr: 0.001
weight_decay: 1e-4
grad_clip_norm: 5.0

# =============================================
# Scheduler
# =============================================
scheduler:
  enabled: true
  name: onecycle
  max_lr: 0.003       # peak LR (보통 base lr의 2~3배)
  pct_start: 0.1      # warmup 비율
  div_factor: 25.0    # initial_lr = max_lr / div_factor
  final_div_factor: 10000.0
  three_phase: false
  anneal_strategy: cos
# =============================================
# Model
# =============================================
embedding_dim: 128
model:
  name: qin_v9ish # options: qin_like | qin_v9ish
  hidden_dim: 256 # used in qin_like MLP
  qnn_hidden: 256 # used in qin_like
  qnn_heads: 4 # used in qin_like
  dropout: 0.2
  attn_dim: 256 # 2 * embedding_dim

  # sparse target attention (qin_like)
  attn_topk: 30

  # qin_v9ish (closer to original QIN)
  qnn:
    num_layers: 4
    num_row: 4
    net_dropout: 0.2
    batch_norm: True
  attn_use_scale: true # scaled dot-product (qin_v9ish)
  attn_relu: true # ReLU gating instead of softmax (qin_v9ish)

# =============================================
# Calibration
# =============================================
calibration:
  temperature_scaling: true
  lr: 0.05
  max_iter: 1000
  use_wll_weights: true
# =============================================
# Prediction
# =============================================
pred_chunk_rows: 100000
pred_output_csv: ./outputs/submission.csv
# =============================================
# Logging
# =============================================
log_interval: 100
save_every_epoch: true

# Weights & Biases (wandb) configuration
wandb:
  enabled: true
  project: "ctr-prediction"  # wandb 프로젝트 이름
  entity: null  # wandb 팀/사용자 이름 (null이면 개인 계정)
  name: null  # 실험 이름 (null이면 자동 생성)
  tags: ["qin", "ctr", "baseline"]  # 실험 태그들
  notes: "QIN-based CTR prediction model"  # 실험 설명
  log_model: false  # 모델 아티팩트 저장 여부