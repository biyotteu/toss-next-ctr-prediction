# =============================================
# Experiment / Paths
# =============================================
exp_name: qin_ctr_baseline
seed: 42
output_dir: ./outputs
artifacts_dir: ./artifacts

paths:
  train_parquet: ./data/train.parquet
  test_parquet: ./data/test.parquet
  sample_submission: ./data/sample_submission.csv
# =============================================
# Data & Features
# =============================================
label_col: clicked
id_col: ID
seq_col: seq # comma‑separated ints as string
# If l_feat_14 is missing or sparse, fallback will be inventory_id
target_feature: l_feat_14
seq_max_len: 256
seq_vocab_size: 200000 # hashing buckets for seq tokens
# Drop constants / bad cols (I/O optimization)
force_drop_cols: [l_feat_20, l_feat_23]
# Numeric/Category handling
numeric_fillna: 0.0
category_hash_buckets: 100000 # hashing trick buckets for generic categorical columns

# =============================================
# Split (leakage‑free options)
# =============================================
split:
  method: stratified # one of: stratified, group_user, group_session, time
  group_key: user_id # used when method=group_user
  session_key: session_id # used when method=group_session
  time_col: event_time # pandas‑parsable datetime col when method=time
  time_val_ratio: 0.1 # last X% as validation if method=time

# (kept for backward compatibility)
val_ratio: 0.1
stratify_label: true
# =============================================
# Imbalance & Sampling
# =============================================
# Option A: pos_weight for BCEWithLogitsLoss (no downsampling)
use_neg_downsampling: true
neg_downsample_ratio: 0.2 # keep 20% of negatives within each batch (~1:10 pos:neg if base pos≈1.9%)
pos_weight: null # if null, auto‑compute by global class ratio
min_pos_per_batch: 16

# Hard Negative Mining (OHEM‑style)
hnm:
  enable: false
  top_neg_frac: 0.2 # keep hardest 20% negatives (by per‑example loss) in the batch
  min_neg: 256 # at least this many negatives kept if available

# =============================================
# DataLoader
# =============================================
persistent_workers: true
prefetch_factor: 8
num_workers: 16
pin_memory: true

# =============================================
# Optimization
# =============================================
batch_size: 8192    #4096 8192
epochs: 100
lr: 0.002
weight_decay: 0.0
grad_clip_norm: 5.0
# =============================================
# Model
# =============================================
embedding_dim: 128
model:
  name: qin_v9ish # options: qin_like | qin_v9ish
  hidden_dim: 256 # used in qin_like MLP
  qnn_hidden: 256 # used in qin_like
  qnn_heads: 4 # used in qin_like
  dropout: 0.1
  attn_dim: 256

  # sparse target attention (qin_like)
  attn_topk: 30

  # qin_v9ish (closer to original QIN)
  qnn:
    num_layers: 4
    num_row: 4
    net_dropout: 0.1
    batch_norm: True
  attn_use_scale: true # scaled dot-product (qin_v9ish)
  attn_relu: true # ReLU gating instead of softmax (qin_v9ish)

# =============================================
# Calibration
# =============================================
calibration:
  temperature_scaling: true
  lr: 0.05
  max_iter: 1000
  use_wll_weights: true
# =============================================
# Prediction
# =============================================
pred_chunk_rows: 100000
pred_output_csv: ./outputs/submission.csv
# =============================================
# Logging
# =============================================
log_interval: 100
save_every_epoch: true